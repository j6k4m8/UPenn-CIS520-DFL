Studying the actions and motion of living animals is a highly desireable capability in neuroscience as well as other behavior biology sciences. But tracking motion in an environment in which animals are free to move without tracking beacons, markers, or other cues can be quite difficult. In recent years, researchers have applied deep learning approaches to 2D video with decent success. These networks, however, require substantial training data, which is cost- and labor-expensive to generate, as it currently requires human annotation.

In this work, we present a multi-network approach to generating and augmenting training data for pose-estimation deep learning models. The first, generative component of our network takes as input a video sequence of 2D RGB images and an angle-offset, and produces a new view of a 3D scene as the network expects it to look from a new camera angle. The second component of our network takes as input a 2D video sequence of an animal performing some action and produces a set of tracking coordinates for points of interest on the animal's body. We propose that due to the 3D geometrical information present in the layers of the first component, the second component benefits from additional priors about the animal's body-shape and posture, which we claim improves the performance of the system in producing 3D track information.

We share benchmarking results that demonstrate that our model performs competitively with state-of-the-art pose-estimation networks, with a substantial reduction in the required volume of human-annotated video data. Furthermore, we illustrate that our model can be easily extended to track pose of other species with minimal fine-tuning and retraining.
